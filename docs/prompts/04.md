ok and all the todos are up to date? we do want to make sure that we have the cli tools
  setup for tracking our scraping and sources in SQLlite for when we begin doing our
  country research with LLM and brave search and/or playwright mcp. and of course we will
  want initial schemas setup for those initial tables. evidence and artifacts downloaded
  need their own table too right? Also there is an interesting thing here. sources may have
  downloadable things like PDFs which would be artifacts but information and knowledge on
  residency and citizenship may be in plain text and could be directly scraped... is that
  an artifact as well? like a markdown knowledge base file? we would organize and index
  that file locally? or would that text information live in the sqlite database? We would
  want all knowledge to be easily accessable without using the database all the time so
  maybe mardown files in an obsidian knowledge base vault but with entries of where each
  one is located and other metadata in a sqlite artifacts database sounds like a good
  compromise 
