ok great. In addition there's some other considerations.

We may use llms directly with brave and playwright to do the 'scraping'. But I want the logging and audit trail of all the scraping we do... sources, artifacts/evidence locally to be recorded without any omissions.

With a plain script this would be easy... but with LLMs we are relying on the LLM to 'remember' to use the seperate tool to log all sources and sites visited into our sqlite sources table

in fact we may need a sources table and also a scraper audit trail table that records not just sources but all pages visited... where sources would be the table that has sites where actual residency and citizenship based knowledge or documents are sourced from directly... but then again we may also want to record pages that we visited on the way leading up to getting to that page so that the workflow is reproducible by a human or another LLM. You see what I am getting at here?

so at the very least we will need some cli tools that LLMs can use to insert data into these tables 


but long term we may want to think about wrapping brave search and playwright MCP servers so that we can automatically log the data as a matter of course.

See the below research into thisd topic that I already did

[sqlite scraper llm integration](../refs/scraper_audit_logging_research.md)

This long term strat can be put into the back log and deprioritized


